# transformers推理

## 非量化模型

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
model_path = "/data/gongoubo/Qwen-1.5-Factory/model_hub/LLM-Research/Meta-Llama-3___1-70B-Instruct"
device = "cuda" # the device to load the model onto

config = AutoConfig.from_pretrained(model_path)

# config.attn_implementation = 'flash_attention_2'

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="flash_attention_2"
)
print(model.device)
model.eval()
tokenizer = AutoTokenizer.from_pretrained(model_path, config=config)

import time

prompt = """请对以下文本进行续写。
文本：视觉-语言预训练（VLP）模型在各种下游任务中表现出了显著的性能。它们的成功在很大程度上依赖于预训练的跨模态数据集的规模。然而，由于缺乏大规模的数据集和中文基准，阻碍了中文VLP模型的发展和更广泛的多语言应用。在这项工作中，我们发布了一个名为 "悟空 "的大规模中文跨模态数据集，其中包含了从网络上收集的1亿个中文图像-文本对。"""
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt")

input_ids = model_inputs.input_ids.to(model.device)

# print(input_ids)

with torch.no_grad():
    t1 = time.time()
    generated_ids = model.generate(
        input_ids,
        max_new_tokens=512,
        temperature=1,
        top_p=1.0,
        top_k=1,
        do_sample=False,
        use_cache=True
    )
    t2 = time.time()
    print(t2-t1)
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    
    total_tokens = len(generated_ids[0].detach().cpu().numpy().tolist())
    print("输入总token数：", len(input_ids[0].detach().cpu().numpy().tolist()))
    print("输出总tokens数：", total_tokens)
    print("输出耗时：{}s".format(t2-t1))
    print("输出tokens/s：", total_tokens / (t2-t1))
    
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    print(response)
```

## AWQ量化模型

pip3 install --upgrade "autoawq>=0.1.6" "transformers>=4.35.0"

```python
# 测试Meta-Llama-3___1-70B-Instruct
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
# model_path = "/data/gongoubo/Qwen-1.5-Factory/model_hub/LLM-Research/Meta-Llama-3___1-70B-Instruct"
model_path = "/data/gongoubo/test_llm/model_hub/llm-research/meta-llama-3___1-70b-instruct-awq-int4"
device = "cuda" # the device to load the model onto

config = AutoConfig.from_pretrained(model_path)

# config.attn_implementation = 'flash_attention_2'

model = AutoModelForCausalLM.from_pretrained(
    model_path,
#     torch_dtype=torch.float16,
    device_map="auto",
#     attn_implementation="flash_attention_2"
)
print(model.device)
model.eval()
tokenizer = AutoTokenizer.from_pretrained(model_path, config=config)

import time

prompt = """请对以下文本进行续写。
文本：视觉-语言预训练（VLP）模型在各种下游任务中表现出了显著的性能。它们的成功在很大程度上依赖于预训练的跨模态数据集的规模。然而，由于缺乏大规模的数据集和中文基准，阻碍了中文VLP模型的发展和更广泛的多语言应用。在这项工作中，我们发布了一个名为 "悟空 "的大规模中文跨模态数据集，其中包含了从网络上收集的1亿个中文图像-文本对。"""

# prompt = """孙悟空（又称美猴王、齐天大圣、孙行者、斗战胜佛），是中国古典神魔小说《西游记》中的人物。由开天辟地产生的仙石孕育而生，出生地位于东胜神洲的花果山上，因带领猴群进入水帘洞而被尊为“美猴王”。 [61]为了学艺而漂洋过海拜师于须菩提祖师，得名“孙悟空”， [63]学会大品天仙诀、七十二变 [263]、筋斗云等高超的法术。 [64-66]
# 出师的孙悟空未得兵器，从四海龙王处取得如意金箍棒、凤翅紫金冠、锁子黄金甲、藕丝步云履， [70]后大闹地府勾去生死簿， [3]惊动上天而被玉皇大帝招安，封为弼马温。 [75]得知职位低卑后怒返花果山， [76]并战胜李天王和哪吒三太子的讨伐， [78]迫使玉帝封其为齐天大圣，并在天宫建有府邸，奉旨管理蟠桃园。 [79]因搅乱王母的蟠桃盛会、偷吃太上老君的金丹， [81]炼成了金钢之躯， [89]在太上老君的八卦炉中被熏成火眼金睛。 [90]之后大闹天宫，十万天兵天将、四大天王、二十八星宿等对其围剿亦不能将其击败， [91]后来在与如来佛祖的打赌斗法中失利，被压在如来用五指化作的五行山下悔过自新。 [93]
# 五百余年后经观音点化，被唐僧救出，法号行者，保护唐僧西天取经， [95]一路降妖除魔，不畏艰难困苦，历经九九八十一难，最后取得真经修成正果，被封为斗战胜佛。 [182]
# 孙悟空生性聪明、活泼、忠诚，在民间文化中代表了机智与勇敢，被中国人奉为斩妖除魔的英雄人物。 [210]自唐代出现西游传说，孙悟空的形象经历过多番创作，一直延续至明代出现了这些传说的集大成本——《西游记》，将孙悟空的形象塑造推向了巅峰。 [211]请对上面文本进行续写。
# """
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": prompt}
]

# messages = [
#     {"role": "system", "content": "You are a helpful assistant."},
#     {"role": "user", "content": prompt}
# ]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt")

input_ids = model_inputs.input_ids.to(model.device)

# print(input_ids)

with torch.no_grad():
    t1 = time.time()
    generated_ids = model.generate(
        input_ids,
        max_new_tokens=1,
        temperature=1,
        top_p=1.0,
        top_k=1,
        do_sample=False,
        use_cache=True
    )
    t2 = time.time()
    print(t2-t1)
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    
    total_tokens = len(generated_ids[0].detach().cpu().numpy().tolist())
    print("输入总token数：", len(input_ids[0].detach().cpu().numpy().tolist()))
    print("输出总tokens数：", total_tokens)
    print("输出耗时：{}s".format(t2-t1))
    print("输出tokens/s：", total_tokens / (t2-t1))
    
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    print(response)
```

- use_cache：用于控制是否使用KV-cache (Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.)
- attn_implementation：用于控制使用flash-attn2 （attn_implementation (`str`, *optional*) — The attention implementation to use in the model. Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (attention using `torch.nn.functional.scaled_dot_product_attention`), or `"flash_attention_2"` (attention using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.）

# TGI推理

启动服务：

```shell
docker run -it --rm --gpus all -p 80:80 --shm-size 64g  -v /data:/data  ghcr.io/huggingface/text-generation-inference:2.2.0  --model-id "/data/gongoubo/Qwen-1.5-Factory/model_hub/LLM-Research/Meta-Llama-3___1-70B-Instruct" --dtype float16 --num-shard 8

docker run -it --rm --gpus all -p 80:80 --shm-size 64g  -v /data:/data  ghcr.io/huggingface/text-generation-inference:2.2.0  --model-id /data/gongoubo/test_llm/model_hub/llm-research/meta-llama-3___1-70b-instruct-awq-int4 --quantize awq --num-shard 8

docker run -it --rm --gpus "device=0" -p 13000:80 --shm-size 64g  -v /data:/data  ghcr.io/huggingface/text-generation-inference:2.2.0  --model-id /data/gongoubo/test_llm/model_hub/qwen/Qwen1___5-7B-Chat-AWQ --quantize awq

docker run -it --rm --gpus all -p 80:80 --shm-size 64g  -v /data:/data  ghcr.io/huggingface/text-generation-inference:2.2.0  --model-id "/data/gongoubo/test_llm/model_hub/qwen/qwen2-57b-a14b-instruct" --dtype float16

docker run -it --rm --gpus all -p 80:80 --shm-size 64g  -v /data:/data  ghcr.io/huggingface/text-generation-inference:2.2.0  --model-id /data/gongoubo/test_llm/model_hub/model_hub/ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ --quantize awq --num-shard 8
```

调用接口：

```python



import requests
import json
from pprint import pprint


def get_requests_greedy(inputs):
    # url = 'http://192.168.120.100:23825/generate'
    url = "http://192.168.16.6:80/generate"
    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
    data = {
        "inputs": inputs,
        "parameters": {
            "best_of": 1,
            "temperature": None,
            "repetition_penalty": None,
            "top_k": None,
            "top_p": None,
            "do_sample": False,
            "return_full_text": False,
            "stop": [],
            "truncate": None,
            "watermark": False,
            "details": False,
            "decode_input_details": False,
            "seed": None,
            "max_new_tokens": 512
        }
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))
    # print(response.content)
    result = response.json()
    # pprint(result)
    return result


if __name__ == '__main__':


    prompt = """请对以下文本进行续写。
    文本：视觉-语言预训练（VLP）模型在各种下游任务中表现出了显著的性能。它们的成功在很大程度上依赖于预训练的跨模态数据集的规模。然而，由于缺乏大规模的数据集和中文基准，阻碍了中文VLP模型的发展和更广泛的多语言应用。在这项工作中，我们发布了一个名为 "悟空 "的大规模中文跨模态数据集，其中包含了从网络上收集的1亿个中文图像-文本对。"""
    
    
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
    model_path = "/data/gongoubo/Qwen-1.5-Factory/model_hub/LLM-Research/Meta-Llama-3___1-70B-Instruct"

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": prompt}
]

    inp = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    import time
    t1 = time.time()
    output = get_requests_greedy(inp)
    t2 = time.time()
    print(output["generated_text"])
    

    
    print(t2-t1)
    tokens =model_inputs = tokenizer([output["generated_text"]])
    input_ids = model_inputs.input_ids
    print(len(input_ids[0]))
    print(len(input_ids[0]) / (t2-t1))


import requests
import json
from pprint import pprint


def get_requests_greedy(inputs):
    # url = 'http://192.168.120.100:23825/generate'
    url = "http://192.168.16.6:80/generate"
    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
    data = {
        "inputs": inputs,
        "parameters": {
            "best_of": 1,
            "temperature": None,
            "repetition_penalty": None,
            "top_k": None,
            "top_p": None,
            "do_sample": False,
            "return_full_text": False,
            "stop": [],
            "truncate": None,
            "watermark": False,
            "details": False,
            "decode_input_details": False,
            "seed": None,
            "max_new_tokens": 512
        }
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))
    # print(response.content)
    result = response.json()
    # pprint(result)
    return result


if __name__ == '__main__':


    prompt = """请对以下文本进行续写。
    文本：视觉-语言预训练（VLP）模型在各种下游任务中表现出了显著的性能。它们的成功在很大程度上依赖于预训练的跨模态数据集的规模。然而，由于缺乏大规模的数据集和中文基准，阻碍了中文VLP模型的发展和更广泛的多语言应用。在这项工作中，我们发布了一个名为 "悟空 "的大规模中文跨模态数据集，其中包含了从网络上收集的1亿个中文图像-文本对。"""
    
    
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
    model_path = "/data/gongoubo/Qwen-1.5-Factory/model_hub/LLM-Research/Meta-Llama-3___1-70B-Instruct"

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": prompt}
]

    inp = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    import time
    t1 = time.time()
    output = get_requests_greedy(inp)
    t2 = time.time()
    print(output["generated_text"])
    

    
    print(t2-t1)
    tokens = model_inputs = tokenizer([output["generated_text"]])
    input_ids = model_inputs.input_ids
    print(len(input_ids[0]))
    print(len(input_ids[0]) / (t2-t1))
```

# vllm推理

vllm部署：

```shell

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m vllm.entrypoints.openai.api_server --model /data/gongoubo/test_llm/model_hub/llm-research/meta-llama-3___1-70b-instruct-awq-int4 --gpu-memory-utilization 0.8 --max-model-len 888 --dtype "auto" --trust-remote-code --tensor-parallel-size 8

python -m vllm.entrypoints.openai.api_server --model /data/gongoubo/test_llm/model_hub/qwen/Qwen1___5-7B-Chat-AWQ --quantization awq --tensor-parallel-size 1 --max-model-len 4096

python -m vllm.entrypoints.openai.api_server --model /data/gongoubo/test_llm/model_hub/model_hub/models--ybelkada--Mixtral-8x7B-Instruct-v0.1-AWQ/snapshots/210178de52e8050f8082724b7e7ab8944c04cdce --quantization awq_marlin --tensor-parallel-size 8
```

vllm推理：

```python
# 测试vllm的推理效率


import requests
import json
from pprint import pprint


def get_requests_greedy(input):
    # url = 'http://192.168.120.100:23825/generate'
    url = "http://192.168.16.6:8000/v1/completions"
    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
    data = {
#         "model": "/data/gongoubo/test_llm/model_hub/model_hub/models--ybelkada--Mixtral-8x7B-Instruct-v0.1-AWQ/snapshots/210178de52e8050f8082724b7e7ab8944c04cdce",
#         "model": "/data/gongoubo/test_llm/model_hub/qwen/Qwen1___5-7B-Chat-AWQ",
        "model": "/data/gongoubo/test_llm/model_hub/llm-research/meta-llama-3___1-70b-instruct-awq-int4",
        "prompt": input,
        "max_tokens": 512,
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))
    # print(response.content)
    result = response.json()
    # pprint(result)
    return result

if __name__ == '__main__':


    prompt = """请对以下文本进行续写。
    文本：视觉-语言预训练（VLP）模型在各种下游任务中表现出了显著的性能。它们的成功在很大程度上依赖于预训练的跨模态数据集的规模。然而，由于缺乏大规模的数据集和中文基准，阻碍了中文VLP模型的发展和更广泛的多语言应用。在这项工作中，我们发布了一个名为 "悟空 "的大规模中文跨模态数据集，其中包含了从网络上收集的1亿个中文图像-文本对。"""
    
    
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
#     model_path = "/data/gongoubo/Qwen-1.5-Factory/model_hub/LLM-Research/Meta-Llama-3___1-70B-Instruct"
    model_path = "/data/gongoubo/test_llm/model_hub/llm-research/meta-llama-3___1-70b-instruct-awq-int4"
#     model_path = "/data/gongoubo/test_llm/model_hub/qwen/qwen2-57b-a14b-instruct"
#     model_path = "/data/gongoubo/test_llm/model_hub/qwen/Qwen1___5-7B-Chat-AWQ"
#     model_path = "/data/gongoubo/test_llm/model_hub/model_hub/models--ybelkada--Mixtral-8x7B-Instruct-v0.1-AWQ/snapshots/210178de52e8050f8082724b7e7ab8944c04cdce"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
#     inp = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""".format(prompt)
    
#     messages = [
#         {"role": "system", "content": "You are a helpful assistant."},
#         {"role": "user", "content": prompt}
#     ]
#     prompt = "who are you"
    
    messages = [
        {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
#         {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]

    inp = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    print(inp)
    import time
    t1 = time.time()
    output = get_requests_greedy(inp)
    t2 = time.time()
    print(output)

    
    print(t2-t1)
#     tokens =model_inputs = tokenizer([output["generated_text"]])
#     input_ids = model_inputs.input_ids
#     print(len(input_ids[0]))
    print(output["usage"]["completion_tokens"] / (t2-t1))
```

# 部分结果

| 模型名称                             | 输入token数 | 预设输出token数 | 实际输出token数 | 输出耗时s | 输出tokens/s | 是否使用flash-attn2 | 是否使用kv-cache | 推理方式    |
| ------------------------------------ | ----------- | --------------- | --------------- | --------- | ------------ | ------------------- | ---------------- | ----------- |
| Meta-Llama-3.1-70B-Instruct          | 165         | 1               | 1               | 3.88      | /            | 否                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct          | 165         | 1               | 1               | 3.99      | /            | 是                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct          | 165         | 512             | 162             | 31.67     | 5.11         | 否                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct          | 165         | 512             | 162             | 33.71     | 4.8          | 是                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct          | 165         | 1               | 1               | 0.27      | /            | /                   | /                | TGI         |
| Meta-Llama-3.1-70B-Instruct          | 165         | 512             | 191             | 5.91      | 32.28        | /                   | /                | TGI         |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 1               | 1               | 1.77      | /            | 否                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 1               | 1               | 1.41      | /            | 是                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 512             | 141             | 20.42     | 6.9          | 否                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 512             | 141             | 20.6      | 6.84         | 是                  | 是               | huggingface |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 1               | 1               | 0.22      | /            | /                   | /                | TGI         |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 512             | 129             | 2.45      | 52.57        | /                   | /                | TGI         |
| Meta-Llama-3.1-70B-Instruct-AWQ-Int4 | 186         | 512             | 180             | 3.51      | 51.14        | /                   | /                | vllm        |
| Qwen1.5-7B-Chat-AWQ                  | 130         | 1               | 1               | 0.53      | /            | 否                  | 是               | huggingface |
| Qwen1.5-7B-Chat-AWQ                  | 130         | 1               | 1               | 0.53      | /            | 是                  | 是               | huggingface |
| Qwen1.5-7B-Chat-AWQ                  | 130         | 512             | 201             | 7.1       | 28.27        | 否                  | 是               | huggingface |
| Qwen1.5-7B-Chat-AWQ                  | 130         | 512             | 201             | 7.07      | 28.4         | 是                  | 是               | huggingface |
| Qwen1.5-7B-Chat-AWQ                  | 165         | 1               | 1               | 0.04      | /            | /                   | /                | TGI         |
| Qwen1.5-7B-Chat-AWQ                  | 165         | 512             | 243             | 1.89      | 128.18       | /                   | /                | TGI         |
| Qwen1.5-7B-Chat-AWQ                  | 165         | 512             | 275             | 2.71      | 101.36       | /                   | /                | vllm        |
| qwen2-57b-a14b-instruct              | 130         | 1               | 1               | 4.37      | /            | 否                  | 是               | huggingface |
| qwen2-57b-a14b-instruct              | 130         | 1               | 1               | 4.24      | /            | 是                  | 是               | huggingface |
| qwen2-57b-a14b-instruct              | 130         | 512             | 39              | 21.52     | 1.81         | 否                  | 是               | huggingface |
| qwen2-57b-a14b-instruct              | 130         | 512             | 287             | 132.27    | 2.16         | 是                  | 是               | huggingface |
| qwen2-57b-a14b-instruct              | 130         | 1               | 1               | 0.65      | /            | /                   | /                | TGI         |
| qwen2-57b-a14b-instruct              | 130         | 512             | 311             | 139       | 2.23         | /                   | /                | TGI         |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 1               | 1               | 3.32      | /            | 否                  | 是               | huggingface |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 1               | 1               | 3.55      | /            | 是                  | 是               | huggingface |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 512             | 512             | 46.97     | 10.89        | 否                  | 是               | huggingface |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 512             | 512             | 48.7      | 10.51        | 是                  | 是               | huggingface |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 1               | 1               | 0.16      | /            | /                   | /                | TGI         |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 512             | 334             | 5.02      | 66.43        | /                   | /                | TGI         |
| Mixtral-8x7B-Instruct-v0.1-AWQ       | 191         | 512             | 512             | 6.36      | 80.47        | /                   | /                | vllm        |

## 为什么HF的推理速度这么慢？

因为hf推理采用pipeline的方式，将模型的不同的层切分到不同的GPU上。